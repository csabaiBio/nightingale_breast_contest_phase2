{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd692b4",
   "metadata": {},
   "source": [
    "Breast cancer stage prediction from pathological whole slide images with hierarchical image pyramid transformers.\n",
    "Project developed under the \"High Risk Breast Cancer Prediction Contest Phase 2\" \n",
    "by Nightingale, Association for Health Learning & Inference (AHLI)\n",
    "and Providence St. Joseph Health\n",
    "\n",
    "Parts of code were took over and adapted from HIPT library.\n",
    "\n",
    "https://github.com/mahmoodlab/HIPT/tree/master/2-Weakly-Supervised-Subtyping/utils\n",
    "\n",
    "Copyright (C) 2023 Zsolt Bedohazi, Andras Biricz, Istvan Csabai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc72387-f449-4095-bea0-e3f7eb501355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "from model_hierarchical_mil_stage3_vit_level1 import HIPT_LGP_FC_STAGE3ONLY, Attn_Net_Gated\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc as calc_auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01590962-0674-40c8-b9b3-24489dd46702",
   "metadata": {},
   "source": [
    "### Functions and classes adapted from HIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b74dd-b1ee-454c-b01f-c7f3141db9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_biopsy_subset(labels, minority_class_ratio=0.2, rnd_seed=38):\n",
    "    # set random seed as given\n",
    "    np.random.seed(rnd_seed)\n",
    "    \n",
    "    # collect selected biopsies that will be in the balanced subset\n",
    "    test_local_idx = []\n",
    "    \n",
    "    # get current class occurences for biopsy\n",
    "    class_occurence = np.array(list(dict( Counter(labels) ).values()))[ np.argsort(list(dict( Counter(labels) ).keys()))]\n",
    "    #print(class_occurence)\n",
    "    \n",
    "    # calc class weights\n",
    "    class_weights = ( class_occurence / class_occurence.sum() ).astype(np.float32)\n",
    "    class_weights_dict = dict( zip( np.arange(class_weights.shape[0]), class_weights ))\n",
    "    #print(class_weights_dict)\n",
    "    \n",
    "    # how many of biopsies to include in the balanced subset\n",
    "    nr_class_test = int(labels.shape[0]*np.min(class_weights)*minority_class_ratio)\n",
    "\n",
    "    # collect biopsy indices for the balanced subset\n",
    "    for s in np.unique(labels): #loop over labelss\n",
    "        s_idx = np.arange(labels.shape[0])[labels == s]\n",
    "        rnd_idx = np.random.permutation(s_idx.shape[0])\n",
    "        test_local_idx.append(s_idx[rnd_idx[:nr_class_test]])\n",
    "\n",
    "    # aggregate all the balanced subset's indices\n",
    "    test_idx = np.concatenate(test_local_idx)\n",
    "    \n",
    "    random.Random(23).shuffle(test_idx) # shuffle otherwise lables are ordered\n",
    "    \n",
    "    # other indices not in balanced set will be the rest\n",
    "    train_idx = np.arange(labels.shape[0])[~np.in1d(np.arange(labels.shape[0]), test_local_idx)]\n",
    "    \n",
    "    return train_idx, test_idx#, label_remaining[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efef27-a586-434e-9bcc-835a7d202411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_back_balanced_training_fold( X_current, y_current,\n",
    "                                      minority_class_ratio=0.5, rnd_seed=12 ):\n",
    "    \n",
    "    _, test_idx, = create_balanced_biopsy_subset(y_current,\n",
    "                                                 minority_class_ratio,\n",
    "                                                 rnd_seed)\n",
    "    X_train_balanced = X_current[test_idx]\n",
    "    y_train_balanced = y_current[test_idx]\n",
    "    #y_train_balanced_oh = lb.transform(y_train_balanced)\n",
    "    #print( X_train_balanced.shape, y_train_balanced_oh.shape )\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83683b41-884d-4a20-84c8-d2940290da14",
   "metadata": {},
   "source": [
    "### Load all data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa352f71-aaa1-4609-9e81-49d9bcc11158",
   "metadata": {},
   "source": [
    "def load_all_data(file_paths):\n",
    "    embeddings_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        data = np.load(file_path)\n",
    "        embeddings = data['embedding']\n",
    "\n",
    "        if embeddings.shape[0] >= 15000:\n",
    "            print(embeddings.shape[0])\n",
    "            continue\n",
    "\n",
    "        # skip empty files:\n",
    "        if embeddings.size == 0:\n",
    "            print(f\"Skipping empty file: {files[p]}\")\n",
    "\n",
    "        labels = data['label'].flatten()[0]\n",
    "\n",
    "        embeddings_all.append(embeddings)\n",
    "        labels_all.append(labels)\n",
    "        \n",
    "    return np.array(embeddings_all, dtype=object), np.array(labels_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d449815-f127-4d62-b76d-137019ededc0",
   "metadata": {},
   "source": [
    "biopsy_embeddings_folder = '/home/ngsci/project/nightingale_breast/Preprocessing/hipt_stage3_biopsy_bag_inputs_with_stage2_finetuned_on_nightingale_v2/' #(files with array of shape Mx192)\n",
    "file_paths = np.array(sorted( [biopsy_embeddings_folder + i for i in os.listdir(biopsy_embeddings_folder) if '.npz' in i ]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce3235ae-62f6-4ae1-bfee-ef450b51392b",
   "metadata": {},
   "source": [
    "embeddings, labels = load_all_data(file_paths)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "047336c5-8b8b-449c-8884-325c9567d649",
   "metadata": {},
   "source": [
    "embeddings.shape, labels.shape, Counter(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55258053-998c-4379-a879-ae9c5fd2eff2",
   "metadata": {},
   "source": [
    "### Generate val set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "359cfa5c-733b-4529-86c5-13d9296065ca",
   "metadata": {},
   "source": [
    "remaining_idx, val_idx = create_balanced_biopsy_subset(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77661abc-6122-4b9c-afa9-bc0f1a5f53bc",
   "metadata": {},
   "source": [
    "X_val = embeddings[val_idx]\n",
    "y_val = labels[val_idx]\n",
    "\n",
    "X_remaining = embeddings[remaining_idx]\n",
    "y_remaining = labels[remaining_idx]\n",
    "\n",
    "X_val.shape, y_val.shape, X_remaining.shape, y_remaining.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2aa3071-6482-40b0-8033-003f3d9729df",
   "metadata": {},
   "source": [
    "# genearet balanced train set\n",
    "X_train, y_train = give_back_balanced_training_fold(X_remaining, y_remaining, rnd_seed=15)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d39ebd-5b71-4312-ac65-60f46dde2ee0",
   "metadata": {},
   "source": [
    "X_train[0].shape, y_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81f21a0b-33cb-4a11-b269-9261c0b05ee0",
   "metadata": {},
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0303414-0985-4450-9703-8855012cea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionsDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 num_classes, \n",
    "                 transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, np.expand_dims(label,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755cc98e-1ac2-4489-9fe6-327a3035a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the dataloader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43894c7-1956-4442-9746-51b129f3564c",
   "metadata": {},
   "source": [
    "train_dataset = CollectionsDataset(data=X_train,\n",
    "                           labels=y_train,\n",
    "                           num_classes=5,\n",
    "                           transform=None)    \n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e35cc9a1-573d-41a2-875d-56c8dcb5ed5a",
   "metadata": {},
   "source": [
    "for batch_idx, (embeddings, labels) in enumerate(train_dataset_loader):\n",
    "    print(f'Batch {batch_idx + 1}:')\n",
    "    print('Embeddings:', embeddings.shape)\n",
    "    print('Labels:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afcd22-775f-43c3-8dff-e4f12b1981ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not modified\n",
    "def train_loop(cur, X_train_all, y_train_all, X_val_all, y_val_all, results_dir, num_epochs, model, n_classes, loss_fn=None, gc=32):  \n",
    "            \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    print('\\nInit optimizer ...', end=' ')\n",
    "    \n",
    "    lr = 8e-5 #9e-5\n",
    "    weight_decay = 1e-6\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=0)\n",
    "    #optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, momentum=0.9, weight_decay=1e-5)  # 1e-4, 1e-5\n",
    "    print('Done!')\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10,15,20,25], gamma=0.8)\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,9], gamma=0.36)\n",
    "    \n",
    "    train_loss_all_epoch = []\n",
    "    val_loss_all_epoch = []\n",
    "    val_auc_all_epoch = []\n",
    "    \n",
    "    # training loop with balanced folds\n",
    "    for epoch in range(0, num_epochs):\n",
    "        #my_lr = scheduler.get_last_lr()\n",
    "        #print('Learning rate:', my_lr)\n",
    "        acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
    "        \n",
    "        # genearet a balanced train set\n",
    "        X_train, y_train = give_back_balanced_training_fold(X_train_all, y_train_all, minority_class_ratio=0.5, rnd_seed=int(epoch*1.5+3*epoch))\n",
    "        \n",
    "\n",
    "        \n",
    "        train_dataset = CollectionsDataset(data=X_train,\n",
    "                                   labels=y_train,\n",
    "                                   num_classes=5,\n",
    "                                   transform=None)        \n",
    "\n",
    "\n",
    "        # create the pytorch data loader\n",
    "        train_dataset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, num_workers=2)\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_error = 0.\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "        # Iterate over data.\n",
    "        for bi, (data, label) in enumerate(train_dataset_loader):\n",
    "            \n",
    "            inputs = data\n",
    "            label = label.squeeze(0)\n",
    "            data = inputs.to(device, dtype=torch.float, non_blocking=True)\n",
    "            label = label.to(device, dtype=torch.long, non_blocking=True)\n",
    "        \n",
    "\n",
    "            logits, Y_prob, Y_hat, _, _ = model(data)\n",
    "            #logits, Y_prob, Y_hat, _, _ = model(x_path=data)\n",
    "            acc_logger.log(Y_hat, label)\n",
    "\n",
    "\n",
    "            loss = loss_fn(logits, label)\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            train_loss += loss_value\n",
    "\n",
    "            #if (bi + 1) % 20 == 0:\n",
    "            #print('batch {}, loss: {:.4f}, label: {}, bag_size: {}'.format(bi, loss_value, label.item(), data.size(0)))\n",
    "\n",
    "            error = calculate_error(Y_hat, label)\n",
    "            train_error += error\n",
    "\n",
    "            loss = loss / gc\n",
    "            loss.backward()\n",
    "\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        #scheduler.step()            \n",
    "\n",
    "        # calculate loss and error for epoch\n",
    "        train_loss /= len(train_dataset_loader)\n",
    "        train_error /= len(train_dataset_loader)\n",
    "\n",
    "        print('\\nEpoch: {}, train_loss: {:.4f}, train_error: {:.4f}'.format(epoch, train_loss, train_error))\n",
    "        for i in range(n_classes):\n",
    "            acc, correct, count = acc_logger.get_summary(i)\n",
    "            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
    "            \n",
    "            \n",
    "        # VALIDATION\n",
    "        val_dataset = CollectionsDataset(data=X_val_all,\n",
    "                                       labels=y_val_all,\n",
    "                                       num_classes=5,\n",
    "                                       transform=None)\n",
    "        \n",
    "        val_dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, num_workers=2)\n",
    "        \n",
    "        stop, val_loss_epoch, auc_epoch, auc_separated = validate(model, val_dataset_loader, n_classes, loss_fn, results_dir)\n",
    "        \n",
    "        os.makedirs(results_dir + f\"cv_{cur}/\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(results_dir, f\"cv_{cur}\", \n",
    "                                                    f\"trainloss_{np.round(train_loss,3)}_valloss_{np.round(val_loss_epoch,3)}_auc_{np.round(auc_epoch,3)}_\"\\\n",
    "                                                    +'_'.join(auc_separated)+\"_checkpoint.pt\"))\n",
    "        \n",
    "        train_loss_all_epoch.append(train_loss)\n",
    "        val_loss_all_epoch.append(val_loss_epoch)\n",
    "        val_auc_all_epoch.append(auc_epoch)\n",
    "        \n",
    "        \n",
    "    # Save training parameters to disk    \n",
    "    param_dict = {'num_epochs': num_epochs,\n",
    "                  'lr': lr,\n",
    "                  'weight_decay': weight_decay,\n",
    "                  'train_loss_all_epoch': train_loss_all_epoch,\n",
    "                  'val_loss_all_epoch': val_loss_all_epoch,\n",
    "                  'val_auc_all_epoch': val_auc_all_epoch}\n",
    " \n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e5527-3581-4478-871f-cd7309c0447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def plot_roc(y_true, y_pred):\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        # try to one-hot encode y_true\n",
    "        y_true = F.one_hot(torch.from_numpy(y_true).to(torch.int64), 5)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for class_ind in range(y_pred.shape[-1]):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        auc = roc_auc_score(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        plt.plot(fpr, tpr, '-', label='AUC : %.3f, label : %d' % (auc, class_ind))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f7d68-213f-4431-8249-8fdcce55a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, n_classes, loss_fn = None, results_dir=None):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
    "    # loader.dataset.update_mode(True)\n",
    "    val_loss = 0.\n",
    "    val_error = 0.\n",
    "    \n",
    "    prob = np.zeros((len(loader), n_classes))\n",
    "    labels = np.zeros(len(loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "\n",
    "            data, label = batch\n",
    "            label = label.squeeze(0)\n",
    "            data = data.to(device, dtype=torch.float, non_blocking=True)\n",
    "            label =  label.to(device, dtype=torch.long, non_blocking=True)\n",
    "            \n",
    "            logits, Y_prob, Y_hat, _, _ = model(data)\n",
    "            #logits, Y_prob, Y_hat, _, _ = model(x_path=data)\n",
    "            acc_logger.log(Y_hat, label)\n",
    "            \n",
    "            loss = loss_fn(logits, label)\n",
    "\n",
    "            prob[batch_idx] = Y_prob.cpu().numpy()\n",
    "            labels[batch_idx] = label.item()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            error = calculate_error(Y_hat, label)\n",
    "            val_error += error\n",
    "            \n",
    "\n",
    "    val_error /= len(loader)\n",
    "    val_loss /= len(loader)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        auc = roc_auc_score(labels, prob[:, 1])\n",
    "\n",
    "    else:\n",
    "        auc = roc_auc_score(labels, prob, multi_class='ovr')\n",
    "        \n",
    "        auc_separated = []\n",
    "        labels_oh = F.one_hot(torch.from_numpy(labels).to(torch.int64), 5)\n",
    "        for class_ind in range(prob.shape[-1]):\n",
    "            fpr, tpr, _ = roc_curve(labels_oh[:, class_ind], prob[:, class_ind])\n",
    "            auc_current = np.round( roc_auc_score(labels_oh[:, class_ind], prob[:, class_ind]), 3 )\n",
    "            auc_separated.append(str(auc_current))\n",
    "        \n",
    "\n",
    "    print('\\nVal Set, val_loss: {:.4f}, val_error: {:.4f}, auc: {:.4f}'.format(val_loss, val_error, auc))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        acc, correct, count = acc_logger.get_summary(i)\n",
    "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
    "        \n",
    "    #print('Max probs: ', np.max(prob, axis=0))\n",
    "    #print('Min probs: ', np.min(prob, axis=0))\n",
    "        \n",
    "    # print roc vurve\n",
    "    print(labels.shape, prob.shape)\n",
    "    plot_roc(labels, prob)\n",
    "\n",
    "\n",
    "    return False, val_loss, auc, auc_separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20a86f-be92-4c1f-b851-98552f359b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Logger(object):\n",
    "    \"\"\"Accuracy logger\"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(Accuracy_Logger, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.data = [{\"count\": 0, \"correct\": 0} for i in range(self.n_classes)]\n",
    "    \n",
    "    def log(self, Y_hat, Y):\n",
    "        Y_hat = int(Y_hat)\n",
    "        Y = int(Y)\n",
    "        self.data[Y][\"count\"] += 1\n",
    "        self.data[Y][\"correct\"] += (Y_hat == Y)\n",
    "    \n",
    "    def log_batch(self, Y_hat, Y):\n",
    "        Y_hat = np.array(Y_hat).astype(int)\n",
    "        Y = np.array(Y).astype(int)\n",
    "        for label_class in np.unique(Y):\n",
    "            cls_mask = Y == label_class\n",
    "            self.data[label_class][\"count\"] += cls_mask.sum()\n",
    "            self.data[label_class][\"correct\"] += (Y_hat[cls_mask] == Y[cls_mask]).sum()\n",
    "    \n",
    "    def get_summary(self, c):\n",
    "        count = self.data[c][\"count\"] \n",
    "        correct = self.data[c][\"correct\"]\n",
    "        \n",
    "        if count == 0: \n",
    "            acc = None\n",
    "        else:\n",
    "            acc = float(correct) / count\n",
    "        \n",
    "        return acc, correct, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea1d41-5158-42a2-9fa0-c19c0c3edce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(Y_hat, Y):\n",
    "    error = 1. - Y_hat.float().eq(Y.float()).float().mean().item()\n",
    "\n",
    "    return error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8358b75-52fd-4035-b86a-902d41e14722",
   "metadata": {},
   "source": [
    "### Training - test with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2a15b-7038-49ed-86a9-60aecfb3e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=7):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28940b-07a6-4369-905f-af0ec901f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#biopsy_embeddings_folder = '/home/ngsci/project/nightingale_breast/Preprocessing/hipt_stage3_biopsy_bag_inputs/' #(files with array of shape Mx192)\n",
    "biopsy_embeddings_folder = '/home/ngsci/project/nightingale_breast_working_development_directory/Preprocessing/biopsy_embeddings/biopsy_bag_vit_xs_embeddings_two-nightingale-finetuned-vits_level1/'\n",
    "\n",
    "file_paths = np.array(sorted( [biopsy_embeddings_folder + i for i in os.listdir(biopsy_embeddings_folder) if '.npz' in i ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06916d-7706-43f6-8bd5-a4867f21f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cv_data(data_df):\n",
    "    embeddings_all = []\n",
    "    labels_all = []\n",
    "        \n",
    "    file_paths = [ biopsy_embeddings_folder + filename + '.npz' for filename in data_df.biopsy_id.values]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        data = np.load(file_path)\n",
    "        embeddings = data['embedding']\n",
    "\n",
    "        \n",
    "        if embeddings.shape[0] >= 15000:\n",
    "            np.random.seed(23)\n",
    "            rand_idx = np.random.permutation(embeddings.shape[0])\n",
    "            \n",
    "            print(f\"Embedding>15k, subsampling...: {embeddings.shape[0]}, label: {data['label']}\")\n",
    "            embeddings = embeddings[rand_idx[:15000]]\n",
    "        \n",
    "\n",
    "        # skip empty files:\n",
    "        if embeddings.size == 0:\n",
    "            print(f\"Skipping empty file: {files[p]}\")\n",
    "            \n",
    "        else:\n",
    "\n",
    "            labels = data['label']\n",
    "\n",
    "            embeddings_all.append(embeddings)\n",
    "            labels_all.append(labels)\n",
    "            \n",
    "    return np.array(embeddings_all, dtype=object), np.array(labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05a5be-644a-4847-9893-3956b453a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_nr = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1561b-bac6-4868-ba59-cb09633d4de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860fd2f-6456-482b-bd74-f515b2fc50dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(folds_nr):\n",
    "    #val_loss_all_epoch = []\n",
    "    #val_auc_all_epoch = []\n",
    "\n",
    "    print(f'\\n ############################ CV-Fold {i} - Balanced training ############################')\n",
    "    seed_torch()\n",
    "    \n",
    "    print('\\nInit loss function...', end=' ')\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print('Done!')\n",
    "    \n",
    "    print('\\nInit Model...', end=' ')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HIPT_LGP_FC_STAGE3ONLY().to(device)\n",
    "    print('Done!')\n",
    "\n",
    "    train_df = pd.read_csv(f'cv_splits_multi_stratified/train_split_multi_stratified_{i}.csv')\n",
    "    val_df = pd.read_csv(f'cv_splits_multi_stratified/val_split_multi_stratified_{i}.csv')\n",
    "    \n",
    "    X_train_all, y_train_all = read_cv_data(train_df)\n",
    "    X_val_all, y_val_all = read_cv_data(val_df)\n",
    "    \n",
    "    \n",
    "    n_classes=5\n",
    "    results_dir='./runs/two-nightingale-finetuned-vits_level1/checkpoints_multi_strat_with_test_set_run7/'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    num_epochs = 70        \n",
    "        \n",
    "    param_dict = train_loop(i, X_train_all, y_train_all, X_val_all, y_val_all, results_dir, num_epochs, model, n_classes, loss_fn, gc=32)\n",
    "    \n",
    "    #val_loss_all_epoch.append(param_dict['val_loss_all_epoch'])\n",
    "    #val_auc_all_epoch.append(param_dict['val_auc_all_epoch'])\n",
    "    \n",
    "\n",
    "    json_data = {'num_epochs': param_dict['num_epochs'],\n",
    "                 'lr': param_dict['lr'],\n",
    "                 'weight_decay': param_dict['weight_decay'],\n",
    "                 'train_loss_all_epoch_cv': param_dict['train_loss_all_epoch'],\n",
    "                 'val_loss_all_epoch_all_cv': param_dict['val_loss_all_epoch'],\n",
    "                 'val_auc_all_epoch_all_cv': param_dict['val_auc_all_epoch'],\n",
    "                 'min_val_loss': np.min(param_dict['val_loss_all_epoch']),\n",
    "                 'max_val_auc': np.max(param_dict['val_auc_all_epoch'])}\n",
    "\n",
    "    # Save training parameters to disk    \n",
    "    with open(results_dir + f\"cv_{i}/\" 'test_params.json', 'w') as file:\n",
    "        json.dump(json_data, file)\n",
    "        \n",
    "    \"\"\"    \n",
    "    # plot curves\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(json_data['train_loss_all_epoch_cv'], \"-b\", label=\"train_loss\")\n",
    "    plt.plot(json_data['val_loss_all_epoch_all_cv'], \"-r\", label=\"val_loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(json_data['val_auc_all_epoch_all_cv'], \"-g\", label=\"val_auc\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    ax1.plot(json_data['train_loss_all_epoch_cv'], \"-b\", label=\"train_loss\")\n",
    "    ax1.plot(json_data['val_loss_all_epoch_all_cv'], \"-r\", label=\"val_loss\")\n",
    "    ax1.set_xlabel('epoch #')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(json_data['val_auc_all_epoch_all_cv'], \"-g\", label=\"val_auc\")\n",
    "    ax2.set_xlabel('epoch #')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e11fa87f-7a47-4c9e-ad07-c7530ee1ed90",
   "metadata": {},
   "source": [
    "## SAVE CODES WITH CHECKPOINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad22703-02b8-4c94-9905-f2b67591a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r ./runs/two-nightingale-finetuned-vits_level1/checkpoints_multi_strat_with_test_set_run7_codes.zip ./hipt_stage3_training_balanced_folds_cross_val_vit_level1.ipynb  ./model_hierarchical_mil_stage3_vit_level1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ded7e-9125-4695-acdb-b90a1be3b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6439853-31aa-43ea-9149-80380cbc5812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e244f69d12f14f842eba482c16e5aadaedc716c5f259416fefc0a6e445229e62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
