{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7731f30e",
   "metadata": {},
   "source": [
    "Breast cancer stage prediction from pathological whole slide images with hierarchical image pyramid transformers.\n",
    "Project developed under the \"High Risk Breast Cancer Prediction Contest Phase 2\" \n",
    "by Nightingale, Association for Health Learning & Inference (AHLI)\n",
    "and Providence St. Joseph Health\n",
    "\n",
    "Parts of code were took over and adapted from HIPT library.\n",
    "\n",
    "https://github.com/mahmoodlab/HIPT/blob/master/HIPT_4K/hipt_4k.py\n",
    "\n",
    "https://github.com/mahmoodlab/HIPT/blob/master/HIPT_4K/hipt_model_utils.py\n",
    "\n",
    "Copyright (C) 2023 Zsolt Bedohazi, Andras Biricz, Istvan Csabai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from geojson import GeoJSON\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "#import shapely\n",
    "#from rtree import index\n",
    "#from shapely.ops import cascaded_union, unary_union\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../HIPT_semicol/HIPT_4K/')\n",
    "import vision_transformer4k as vits4k"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f896b422-ae31-48b6-8770-64a66e536e3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# this needs to be installed again and again after starting a new machine\n",
    "!pip install shapely rtree webdataset einops "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4333308b-a7d7-428f-aaf0-a1ab0156f22e",
   "metadata": {},
   "source": [
    "idx_list = np.append(np.arange(0,files.shape[0],files.shape[0]//7)[:-1], files.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cd59cae-f582-41b3-94b6-635c3b99e684",
   "metadata": {},
   "source": [
    "np.vstack((idx_list[:-1],idx_list[1:])).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78b32ae8",
   "metadata": {},
   "source": [
    "### Locate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/ngsci/resnet50_embeddings_4096region_256times1024_level0_holdout/'\n",
    "#source = '/home/ngsci/resnet50_embeddings_4096region_256times1024_level1_holdout/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0685508-5d94-47d9-96ef-33d9a660e2c6",
   "metadata": {},
   "source": [
    "files = np.array( sorted([ i for i in os.listdir(source) if '.h5' in i ]) )\n",
    "files.shape, files[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b43481",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_fp = os.path.join(source, f'*.npy')\n",
    "files = np.array( sorted( glob.glob(slide_fp) ) )\n",
    "files.shape, files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa3fce-2b0a-44a7-9df6-1dd92c89a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_file(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        coords = f['coords'][()]\n",
    "        imgs = f['features_4k'][()]\n",
    "        return coords, imgs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40fea893-2496-4f85-ba2a-7841f9854bc0",
   "metadata": {},
   "source": [
    "coords, features_4k = load_h5_file(files[0])\n",
    "coords.shape, features_4k.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7a2fd4",
   "metadata": {},
   "source": [
    "#### HIPT model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93b3d1b4-a527-42fb-aef2-527c248a6993",
   "metadata": {},
   "source": [
    "def vit4k_xs(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer4K(\n",
    "        patch_size=patch_size, input_embed_dim=1024, output_embed_dim=192,\n",
    "        depth=6, num_heads=6, mlp_ratio=4, \n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e016915-c5a4-41ea-baff-1f989a856697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vit4k(pretrained_weights, arch='vit4k_xs', device=torch.device('cuda:0'), input_embed_dim=1024):\n",
    "    \"\"\"\n",
    "    Builds ViT-4K Model.\n",
    "    \n",
    "    Args:\n",
    "    - pretrained_weights (str): Path to ViT-4K Model Checkpoint.\n",
    "    - arch (str): Which model architecture.\n",
    "    - device (torch): Torch device to save model.\n",
    "    \n",
    "    Returns:\n",
    "    - model256 (torch.nn): Initialized model.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_key = 'teacher'\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model4k = vits4k.__dict__[arch](num_classes=0)\n",
    "    for p in model4k.parameters():\n",
    "        p.requires_grad = False\n",
    "    model4k.eval()\n",
    "    model4k.to(device)\n",
    "\n",
    "    print('HERE', pretrained_weights, os.path.isfile(pretrained_weights))\n",
    "    \n",
    "    if os.path.isfile(pretrained_weights):\n",
    "        print('ISFILE')\n",
    "        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "        if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "            state_dict = state_dict[checkpoint_key]\n",
    "        # remove `module.` prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        msg = model4k.load_state_dict(state_dict, strict=False)\n",
    "        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n",
    "        \n",
    "    return model4k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417c49f-49d7-4882-9d6f-bd8a707112de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIPT_4K(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    HIPT Model (ViT-4K) for encoding non-square images (with [256 x 256] patch tokens), with \n",
    "    [256 x 256] patch tokens encoded via ViT-256 using [16 x 16] patch tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        #model4k_path: str = '../Checkpoints/vit4k_xs_dino.pth', \n",
    "                 \n",
    "        # stage 2 model trained locally without finetuning on platform\n",
    "        #model4k_path: str = 'nightingale_checkpoint_ViT4096_on_resnet50_embeddings.pth',\n",
    "                 \n",
    "        # stage 2 model trained locally finetuned on platform\n",
    "        model4k_path: str = '/home/ngsci/project/checkpoints_for_hipt_stage3_input_generator_resnet_level0/checkpoint_on_resnet_level0_nofinetune_from_local_bracs_dinoloss1.6_1.6.pth',         \n",
    "        device4k = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model4k = get_vit4k(pretrained_weights=model4k_path).to(device4k)\n",
    "        #self.model4k = get_vit4k(pretrained_weights='None').to(device4k)\n",
    "        self.device4k = device4k\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of HIPT (given an image tensor x), outputting the [CLS] token from ViT-4K.\n",
    "        1. x is center-cropped such that the W / H is divisible by the patch token size in ViT-4K (e.g. - 256 x 256).\n",
    "        2. x then gets unfolded into a \"batch\" of [256 x 256] images.\n",
    "        3. A pretrained ViT-256 model extracts the CLS token from each [256 x 256] image in the batch.\n",
    "        4. These batch-of-features are then reshaped into a 2D feature grid (of width \"w_256\" and height \"h_256\".)\n",
    "        5. This feature grid is then used as the input to ViT-4K, outputting [CLS]_4K.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): [1 x C x W' x H'] image tensor.\n",
    "\n",
    "        Return:\n",
    "            - features_cls4k (torch.Tensor): [1 x 192] cls token (d_4k = 192 by default).\n",
    "        \"\"\"\n",
    "        features_resnet = torch.from_numpy(x) # B x 256 x 1024\n",
    "        features_resnet = features_resnet.transpose(1,2) # B x 1024 x 256\n",
    "        features_resnet = features_resnet.reshape(x.shape[0], 1024, 16, 16) # B, embed_dim, w, h\n",
    "        #print( features_resnet.shape )\n",
    "        features_resnet = features_resnet.to(self.device4k, non_blocking=True)  # 4. [B x 1024 x 16 x 16]\n",
    "        features_cls4k = self.model4k.forward(features_resnet)                  # 5. [B x 192], where 192 == dim of ViT-4K [ClS] token.\n",
    "        return features_cls4k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2966c7-133f-4778-ba71-0db710577a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = HIPT_4K()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b26bd1b9-46fe-4fa9-98db-06a6fc96af68",
   "metadata": {},
   "source": [
    "emb_4k = np.load( files[0] ).astype(np.float32)\n",
    "emb_4k.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff67b2c7-da9a-4d7e-a566-7e0492d42061",
   "metadata": {},
   "source": [
    "%%time\n",
    "preds = model.forward(features_4k.astype(np.float32)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb11310-d0e2-4853-b95d-f55f0bfe627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d33b0f61-402f-4615-bae4-2f7cc6b6db7c",
   "metadata": {},
   "source": [
    "### numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf17df-ec2c-40df-9ca6-5a90e21a90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#destination = 'embeddings/vit_xs_embeddings_nofinetuned_resnet50_embeddings_4096region_256times1024_level0_holdout/'\n",
    "destination = '/home/ngsci/vit_xs_embeddings_nofinetuned_resnet50_embeddings_4096region_256times1024_level0_holdout/'\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "with warnings.catch_warnings(record=True):\n",
    "    #preds_all = []\n",
    "    for p in tqdm( range( 9000, 12500)):#files.shape[0] ) ):\n",
    "        # skip already processed\n",
    "        if not os.path.exists( destination+os.path.basename( files[p] ) ):\n",
    "            emb_4k = np.load( files[p] ).astype(np.float32)\n",
    "            #_, emb_4k = load_h5_file(files[p])\n",
    "            #emb_4k = emb_4k.astype(np.float32)\n",
    "\n",
    "            # skip empty files:\n",
    "            if emb_4k.size == 0:\n",
    "                print(f\"Skipping empty file: {files[p]}\")\n",
    "                continue\n",
    "\n",
    "            preds = model(emb_4k).cpu().numpy().astype(np.float16)\n",
    "            np.save( destination+os.path.basename( files[p].replace('.h5','') ), preds )\n",
    "        else:\n",
    "            pass\n",
    "            #preds_all.append(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7ab38c6-46c8-4baf-9d18-18d603ffeae4",
   "metadata": {},
   "source": [
    "### hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f11e32-d774-493f-b2da-21f552fb9308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "destination = 'vit_xs_embeddings_finetuned_on_bracs_on_top_of_resnet50_embeddings_4096region_256times1024_level1_finetuned_on_nightingale_level0_holdout/'\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "with warnings.catch_warnings(record=True):\n",
    "    #preds_all = []\n",
    "    for p in tqdm( range(4500)):#files.shape[0] ) ):\n",
    "        # skip already processed\n",
    "        if not os.path.exists( destination+os.path.basename( files[p] ) ):\n",
    "            #emb_4k = np.load( files[p] ).astype(np.float32)\n",
    "            _, emb_4k = load_h5_file(files[p])\n",
    "            emb_4k = emb_4k.astype(np.float32)\n",
    "\n",
    "            # skip empty files:\n",
    "            if emb_4k.size == 0:\n",
    "                print(f\"Skipping empty file: {files[p]}\")\n",
    "                continue\n",
    "\n",
    "            preds = model(emb_4k).cpu().numpy().astype(np.float16)\n",
    "            np.save( destination+os.path.basename( files[p].replace('.h5','') ), preds )\n",
    "        else:\n",
    "            pass\n",
    "            #preds_all.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b763e7c-42d3-4f3c-9879-efaf3f101a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1d2b3-997a-47c4-8330-ee6021621882",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c1bc4-cea4-4bfe-bab5-2200fde319ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist( preds[10], bins=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e9751-ca85-41b2-94ee-6bf831b3215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist( preds[30], bins=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86984952-f7d8-40ef-adc6-cbaa3e3dde83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e244f69d12f14f842eba482c16e5aadaedc716c5f259416fefc0a6e445229e62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
